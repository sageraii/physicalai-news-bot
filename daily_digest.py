#!/usr/bin/env python3
"""
daily_digest.py - Physical AI Daily Digest ë©”ì¸ ì‹¤í–‰ íŒŒì¼

Physical AI ê´€ë ¨ RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•˜ì—¬ Slack ì±„ë„ë¡œ ì¼ì¼ ìš”ì•½ì„ ì „ì†¡í•©ë‹ˆë‹¤.
"""

import hashlib
import json
import logging
import os
import sys
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import feedparser
import requests
import yaml
from bs4 import BeautifulSoup

from llm_client import create_llm_client

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def load_dotenv():
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    env_path = Path(__file__).parent / ".env"
    if not env_path.exists():
        return

    with open(env_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                key, _, value = line.partition("=")
                key = key.strip()
                value = value.strip()
                if (value.startswith('"') and value.endswith('"')) or (
                    value.startswith("'") and value.endswith("'")
                ):
                    value = value[1:-1]
                if key not in os.environ:
                    os.environ[key] = value


# ì•± ì‹œì‘ ì‹œ ë¡œë“œ
load_dotenv()


def strip_html(html_content: str) -> str:
    """HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜"""
    if not html_content:
        return ""
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        for tag in soup(["script", "style"]):
            tag.decompose()
        text = soup.get_text(separator=" ", strip=True)
        text = " ".join(text.split())
        return text
    except Exception:
        return html_content


class PhysicalAIDailyDigest:
    """Physical AI Daily Digest ë´‡ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, webhook_url: str, config_path: str = "config.yaml"):
        """
        Args:
            webhook_url: Slack ì›¹í›… URL
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.webhook_url = webhook_url
        self.config = self._load_config(config_path)
        self.feeds = self.config.get("feeds", {})
        self.llm_client = create_llm_client(self.config.get("llm", {}))

        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.system_prompt = self._load_prompt("prompts/system.txt")
        self.batch_prompt_template = self._load_prompt("prompts/translate_summarize_batch.txt")

        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        llm_config = self.config.get("llm", {})
        self.batch_size = llm_config.get("batch_size", 10)

        # ìƒíƒœ íŒŒì¼ ê²½ë¡œ
        self.state_file = Path(__file__).parent / "sent_articles.json"

        # Slack ì„¤ì •
        slack_config = self.config.get("slack", {})
        self.max_articles_per_category = slack_config.get("max_articles_per_category", 5)

        # ìŠ¤ì¼€ì¤„ ì„¤ì •
        schedule_config = self.config.get("schedule", {})
        self.lookback_hours = schedule_config.get("lookback_hours", 24)

        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì„¤ì •
        cat_config = self.config.get("categorization", {})
        self.categorization_enabled = cat_config.get("enabled", False)
        self.categories = cat_config.get("categories", {})

    def _load_config(self, path: str) -> Dict:
        """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_path = Path(__file__).parent / path
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        except FileNotFoundError:
            logger.warning(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            return {}
        except yaml.YAMLError as e:
            logger.error(f"YAML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {}

    def _load_prompt(self, path: str) -> Optional[str]:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        prompt_path = Path(__file__).parent / path
        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            logger.warning(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")
            return None

    def _categorize_article(self, article: Dict, processed: Dict) -> str:
        """ê¸°ì‚¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        if not self.categorization_enabled:
            return "general"

        # LLMì´ ì œì•ˆí•œ ì¹´í…Œê³ ë¦¬ íŒíŠ¸ í™•ì¸
        category_hint = processed.get("category_hint", "")
        if category_hint and category_hint in self.categories:
            return category_hint

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        search_text = " ".join([
            article.get("title", ""),
            article.get("description", ""),
            processed.get("translated_title", ""),
            processed.get("summary", ""),
        ]).lower()

        for cat_key, cat_info in self.categories.items():
            if cat_key == "general":
                continue
            keywords = cat_info.get("keywords", [])
            for keyword in keywords:
                if keyword.lower() in search_text:
                    return cat_key

        return "general"

    def _group_by_category(self, articles: List[Dict]) -> Dict[str, List[Dict]]:
        """ê¸°ì‚¬ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”"""
        grouped = {}
        for item in articles:
            category = item.get("category", "general")
            if category not in grouped:
                grouped[category] = []
            grouped[category].append(item)
        return grouped

    def _load_state(self) -> Dict:
        """ì´ì „ ìƒíƒœ ë¡œë“œ"""
        try:
            if self.state_file.exists():
                with open(self.state_file, "r", encoding="utf-8") as f:
                    state = json.load(f)
                    today = datetime.now().strftime("%Y-%m-%d")
                    if state.get("date") != today:
                        return {"date": today, "sent_today": []}
                    return state
        except (json.JSONDecodeError, IOError) as e:
            logger.warning(f"ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

        return {"date": datetime.now().strftime("%Y-%m-%d"), "sent_today": []}

    def _save_state(self, state: Dict):
        """ìƒíƒœ ì €ì¥"""
        try:
            with open(self.state_file, "w", encoding="utf-8") as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except IOError as e:
            logger.error(f"ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_article_id(self, source: str, url: str) -> str:
        """ê¸°ì‚¬ ê³ ìœ  ID ìƒì„±"""
        return hashlib.md5(f"{source}:{url}".encode()).hexdigest()

    def _parse_published_date(self, entry: Any) -> Optional[datetime]:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ"""
        date_fields = ["published_parsed", "updated_parsed", "created_parsed"]

        for field in date_fields:
            parsed = getattr(entry, field, None)
            if parsed:
                try:
                    return datetime(*parsed[:6], tzinfo=timezone.utc)
                except (TypeError, ValueError):
                    continue

        return None

    def fetch_feeds(self) -> List[Dict]:
        """ëª¨ë“  RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        articles = []
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=self.lookback_hours)
        state = self._load_state()
        sent_ids = set(state.get("sent_today", []))

        for source_name, feed_url in self.feeds.items():
            logger.info(f"í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {source_name}")
            try:
                feed = feedparser.parse(feed_url)

                if feed.bozo and feed.bozo_exception:
                    logger.warning(f"í”¼ë“œ íŒŒì‹± ê²½ê³  ({source_name}): {feed.bozo_exception}")

                for entry in feed.entries[:10]:
                    try:
                        url = entry.get("link", "")
                        article_id = self._generate_article_id(source_name, url)

                        if article_id in sent_ids:
                            continue

                        pub_date = self._parse_published_date(entry)
                        if pub_date and pub_date < cutoff_time:
                            continue

                        description = ""
                        if hasattr(entry, "summary"):
                            description = entry.summary
                        elif hasattr(entry, "description"):
                            description = entry.description

                        articles.append({
                            "id": article_id,
                            "title": entry.get("title", "ì œëª© ì—†ìŒ"),
                            "link": url,
                            "source": source_name,
                            "description": description,
                            "published": pub_date.isoformat() if pub_date else None,
                        })

                    except Exception as e:
                        logger.warning(f"ì—”íŠ¸ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜ ({source_name}): {e}")
                        continue

                time.sleep(0.5)

            except Exception as e:
                logger.error(f"í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ ({source_name}): {e}")
                continue

        logger.info(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")
        return articles

    def _prepare_article_for_batch(self, article: Dict) -> Dict:
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê¸°ì‚¬ ì •ë³´ë¥¼ ì¤€ë¹„"""
        return {
            "title": article["title"],
            "description": strip_html(article.get("description", "") or "")[:500],
            "source": article["source"],
        }

    def _create_fallback_result(self, article: Dict) -> Dict:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ê²°ê³¼ ìƒì„±"""
        clean_desc = strip_html(article.get("description", "") or "")
        return {
            "translated_title": article["title"],
            "summary": clean_desc[:300],
            "category_hint": None,
        }

    def translate_and_summarize_batch(self, articles: List[Dict]) -> List[Dict]:
        """ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ë¡œ ë²ˆì—­ ë° ìš”ì•½"""
        if not articles:
            return []

        if not self.llm_client:
            return [self._create_fallback_result(article) for article in articles]

        if not self.batch_prompt_template:
            logger.warning("ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ì—†ìŠµë‹ˆë‹¤.")
            return [self._create_fallback_result(article) for article in articles]

        # ë°°ì¹˜ìš© ê¸°ì‚¬ ì •ë³´ ì¤€ë¹„
        articles_data = []
        for idx, article in enumerate(articles):
            prepared = self._prepare_article_for_batch(article)
            prepared["index"] = idx
            articles_data.append(prepared)

        articles_json = json.dumps(articles_data, ensure_ascii=False, indent=2)
        prompt = self.batch_prompt_template.format(articles_json=articles_json)

        try:
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {len(articles)}ê°œ ê¸°ì‚¬")
            response = self.llm_client.generate(prompt=prompt, system_prompt=self.system_prompt)

            # JSON ì¶”ì¶œ
            json_str = response.strip()
            if json_str.startswith("```"):
                lines = json_str.split("\n")
                json_str = "\n".join(lines[1:])
                if json_str.endswith("```"):
                    json_str = json_str[:-3]
                json_str = json_str.strip()

            results = json.loads(json_str)

            if not isinstance(results, list):
                logger.warning("ë°°ì¹˜ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return [self._create_fallback_result(article) for article in articles]

            # article_index ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_results = [None] * len(articles)
            for result in results:
                idx = result.get("article_index", -1)
                if 0 <= idx < len(articles):
                    sorted_results[idx] = result

            for idx, result in enumerate(sorted_results):
                if result is None:
                    logger.warning(f"ê¸°ì‚¬ {idx} ê²°ê³¼ ëˆ„ë½, í´ë°± ì²˜ë¦¬")
                    sorted_results[idx] = self._create_fallback_result(articles[idx])

            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            return sorted_results

        except json.JSONDecodeError as e:
            logger.warning(f"ë°°ì¹˜ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.warning(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        return [self._create_fallback_result(article) for article in articles]

    def create_slack_block(self, article: Dict, processed: Dict) -> Dict:
        """Slack Block ìƒì„±"""
        title = processed.get("translated_title", article["title"])[:150]
        summary = processed.get("summary", "")[:500]
        link = article["link"]
        source = article["source"]

        return {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*<{link}|{title}>*\n{summary}\n_ì¶œì²˜: {source}_"
            }
        }

    def _send_webhook(self, payload: Dict) -> bool:
        """Slack ì›¹í›…ìœ¼ë¡œ í˜ì´ë¡œë“œ ì „ì†¡"""
        try:
            response = requests.post(
                self.webhook_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            if response.status_code == 429:
                retry_after = int(response.headers.get("Retry-After", 5))
                logger.warning(f"Rate limited. {retry_after}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(retry_after)
                response = requests.post(
                    self.webhook_url,
                    json=payload,
                    headers={"Content-Type": "application/json"},
                    timeout=30,
                )

            response.raise_for_status()
            return True

        except requests.RequestException as e:
            logger.error(f"Slack ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False

    def send_to_slack(self, processed_articles: List[Dict]) -> bool:
        """Slackìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡"""
        if not processed_articles:
            logger.info("ì „ì†¡í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True

        # í—¤ë”
        today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"ğŸ¤– Physical AI Daily Digest - {today}",
                    "emoji": True
                }
            },
            {"type": "divider"}
        ]

        if self.categorization_enabled:
            # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
            grouped = self._group_by_category(processed_articles)
            category_order = ["nvidia", "robotics", "airesearch", "research", "industry", "korea", "general"]

            for cat_key in category_order:
                if cat_key not in grouped:
                    continue

                articles = grouped[cat_key][:self.max_articles_per_category]
                if not articles:
                    continue

                cat_info = self.categories.get(cat_key, {})
                cat_name = cat_info.get("name", f"ğŸ“° {cat_key}")

                # ì¹´í…Œê³ ë¦¬ í—¤ë”
                blocks.append({
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"*{cat_name}* ({len(articles)}ê°œ)"
                    }
                })

                # ê¸°ì‚¬ë“¤
                for item in articles:
                    blocks.append(item["block"])

                blocks.append({"type": "divider"})
        else:
            # ì¹´í…Œê³ ë¦¬ ì—†ì´ ì „ì†¡
            for item in processed_articles[:self.max_articles_per_category * 5]:
                blocks.append(item["block"])

        # Slackì€ ë¸”ë¡ 50ê°œ ì œí•œ
        if len(blocks) > 50:
            blocks = blocks[:49]
            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": "_...ë” ë§ì€ ê¸°ì‚¬ê°€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤._"}
            })

        payload = {"blocks": blocks}

        if not self._send_webhook(payload):
            return False

        logger.info(f"Slack ì „ì†¡ ì„±ê³µ: {len(processed_articles)}ê°œ ê¸°ì‚¬")
        return True

    def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë¡œì§"""
        logger.info("Physical AI Daily Digest ì‹œì‘")

        # 1. í”¼ë“œ ìˆ˜ì§‘
        articles = self.fetch_feeds()
        if not articles:
            logger.info("ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. ë²ˆì—­ ë° ìš”ì•½ (ë°°ì¹˜ ì²˜ë¦¬)
        processed_articles = []
        state = self._load_state()
        sent_ids = set(state.get("sent_today", []))

        rate_limit_delay = self.config.get("llm", {}).get("rate_limit_delay", 1)

        for batch_start in range(0, len(articles), self.batch_size):
            batch_end = min(batch_start + self.batch_size, len(articles))
            batch_articles = articles[batch_start:batch_end]

            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {batch_start + 1}-{batch_end}/{len(articles)}")

            try:
                batch_results = self.translate_and_summarize_batch(batch_articles)

                for article, processed in zip(batch_articles, batch_results):
                    block = self.create_slack_block(article, processed)
                    category = self._categorize_article(article, processed)

                    processed_articles.append({
                        "article": article,
                        "processed": processed,
                        "block": block,
                        "category": category,
                    })
                    sent_ids.add(article["id"])

                if batch_end < len(articles) and self.llm_client:
                    logger.info(f"Rate limit ëŒ€ê¸°: {rate_limit_delay}ì´ˆ")
                    time.sleep(rate_limit_delay)

            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                for article in batch_articles:
                    processed = self._create_fallback_result(article)
                    block = self.create_slack_block(article, processed)
                    category = self._categorize_article(article, processed)
                    processed_articles.append({
                        "article": article,
                        "processed": processed,
                        "block": block,
                        "category": category,
                    })
                    sent_ids.add(article["id"])

        # 3. Slack ì „ì†¡
        if processed_articles:
            success = self.send_to_slack(processed_articles)

            if success:
                state["sent_today"] = list(sent_ids)
                self._save_state(state)
                logger.info(f"ì´ {len(processed_articles)}ê°œ ê¸°ì‚¬ ì „ì†¡ ì™„ë£Œ")
            else:
                logger.error("Slack ì „ì†¡ ì‹¤íŒ¨")
        else:
            logger.info("ì²˜ë¦¬ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        logger.info("Physical AI Daily Digest ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    webhook_url = os.environ.get("SLACK_WEBHOOK_URL")
    if not webhook_url:
        logger.error("SLACK_WEBHOOK_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    bot = PhysicalAIDailyDigest(webhook_url=webhook_url)
    bot.run()


if __name__ == "__main__":
    main()
